Setup done. Running python script...
2026-02-28 18:53:09,825 [INFO] Loading: data/preprocessing/test_set_mistral_annotated.csv  →  model 'test_set_mistral_annotated'
2026-02-28 18:53:09,943 [INFO] Loading: data/preprocessing/test_set_llama_annotated.csv  →  model 'test_set_llama_annotated'
2026-02-28 18:53:10,052 [INFO] Loading: data/preprocessing/test_set_deepseek_annotated.csv  →  model 'test_set_deepseek_annotated'
2026-02-28 18:53:10,205 [INFO] Loading: data/preprocessing/test_set_magistral_annotated.csv  →  model 'test_set_magistral_annotated'
2026-02-28 18:53:10,470 [INFO] Merged dataset: 4550 sentences common to all models.
2026-02-28 18:53:10,470 [INFO] Computing metrics for: label_binary
2026-02-28 18:53:10,805 [INFO] Computing metrics for: label_component
2026-02-28 18:53:13,405 [INFO] 
Split results:
  agreed  :    924 sentences  (label_source='auto', ready for test set)
  review  :   3525 sentences  (disagreement — fill PENDING labels)
  errors  :    101 sentences  (NC present  — fill PENDING labels)

======================================================================
  INTER-ANNOTATOR AGREEMENT REPORT
======================================================================

Models compared  : test_set_mistral_annotated, test_set_llama_annotated, test_set_deepseek_annotated, test_set_magistral_annotated
Total sentences  : 4550
  → agreed       :    924  (20.3%)  auto-labeled
  → review       :   3525  (77.5%)  needs manual labels
  → errors (NC)  :    101  (2.2%)  needs manual labels

  ⚠  101 sentences contain NC labels. They are included in the metrics
     below as a separate 'nc' category, which may deflate agreement scores.

----------------------------------------------------------------------
  Label: label_binary
----------------------------------------------------------------------
  Fleiss' Kappa        : 0.2632  [Fair]
  Krippendorff's Alpha : 0.2732  [Fair]

  Pairwise Cohen's Kappa:
    test_set_mistral_annotated     vs test_set_llama_annotated        κ = 0.3366  [Fair]
    test_set_mistral_annotated     vs test_set_deepseek_annotated     κ = 0.3519  [Fair]
    test_set_mistral_annotated     vs test_set_magistral_annotated    κ = 0.4291  [Moderate]
    test_set_llama_annotated       vs test_set_deepseek_annotated     κ = 0.2052  [Fair]
    test_set_llama_annotated       vs test_set_magistral_annotated    κ = 0.2993  [Fair]
    test_set_deepseek_annotated    vs test_set_magistral_annotated    κ = 0.1692  [Slight]

----------------------------------------------------------------------
  Label: label_component
----------------------------------------------------------------------
  Fleiss' Kappa        : 0.2066  [Fair]
  Krippendorff's Alpha : 0.2105  [Fair]

  Pairwise Cohen's Kappa:
    test_set_mistral_annotated     vs test_set_llama_annotated        κ = 0.2870  [Fair]
    test_set_mistral_annotated     vs test_set_deepseek_annotated     κ = 0.3413  [Fair]
    test_set_mistral_annotated     vs test_set_magistral_annotated    κ = 0.3166  [Fair]
    test_set_llama_annotated       vs test_set_deepseek_annotated     κ = 0.2134  [Fair]
    test_set_llama_annotated       vs test_set_magistral_annotated    κ = 0.1459  [Slight]
    test_set_deepseek_annotated    vs test_set_magistral_annotated    κ = 0.1258  [Slight]

======================================================================
  Kappa interpretation (Landis & Koch 1977):
    < 0.00  Poor  |  0.00–0.20  Slight  |  0.21–0.40  Fair
    0.41–0.60  Moderate  |  0.61–0.80  Substantial  |  0.81–1.00  Almost Perfect
======================================================================

  Next steps:
    1. Open review_sentences.csv — fill label_binary_final and
       label_component_final for every PENDING row.
    2. Open errors.csv — do the same.
    3. Run build_test_set.py to merge all three CSVs into
       test_set_final.csv.
======================================================================
2026-02-28 18:53:13,411 [INFO] Report saved → data/preprocessing/iaa/iaa_report.txt
2026-02-28 18:53:13,430 [INFO] Saved     12 rows → data/preprocessing/iaa/iaa_pairwise.csv
2026-02-28 18:53:13,516 [INFO] Saved    924 rows → data/preprocessing/iaa/agreed.csv
2026-02-28 18:53:13,820 [INFO] Saved   3525 rows → data/preprocessing/iaa/review_sentences.csv
2026-02-28 18:53:13,836 [INFO] Saved    101 rows → data/preprocessing/iaa/errors.csv
2026-02-28 18:53:13,836 [INFO] 
Done. Fill PENDING labels in review_sentences.csv and errors.csv, then run build_test_set.py.
Script finished
